{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 22:17:08.488510: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 22:17:09.237440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9765dbb85e9f2965\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9765dbb85e9f2965\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import RandomLayers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#name of model or experiment\n",
    "model_name = \"Q_Model\"\n",
    "log_dir= model_name + \"/runs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,\n",
    "    write_graph=True\n",
    "    )\n",
    "\n",
    "\n",
    "%tensorboard --logdir $log_dir\n",
    "\n",
    "checkpoint_path = model_name + \"/epoch_checkpoints/cp-{epoch:04d}.weights.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   checkpoint_path, verbose=1, save_weights_only=True,\n",
    "   # Save weights, every epoch.\n",
    "   save_freq='epoch')\n",
    "\n",
    "\n",
    "n_epochs = 30   # Number of optimization epochs\n",
    "n_layers = 1    # Number of random layers\n",
    "n_batches = 64     # Size of the batches\n",
    "\n",
    "np.random.seed(0)           # Seed for NumPy random number generator\n",
    "tf.random.set_seed(0)       # Seed for TensorFlow random number generator\n",
    "\n",
    "\n",
    "mnist_dataset = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist_dataset.load_data()\n",
    "\n",
    "\n",
    "tf.config.get_visible_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 22:17:14.051796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6795 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:b3:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalize pixel values within 0 and 1\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "# Add extra dimension for convolution channels\n",
    "train_images = np.array(train_images[..., tf.newaxis], requires_grad=False)\n",
    "test_images = np.array(test_images[..., tf.newaxis], requires_grad=False)\n",
    "\n",
    "n_qubits = 4\n",
    "\n",
    "rand_params = np.random.uniform(high=2 * np.pi, size=(n_layers, 4))\n",
    "\n",
    "dev = qml.device(\"default.qubit.tf\", wires=n_qubits)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.get_custom_objects().clear()\n",
    "\n",
    "@keras.utils.register_keras_serializable(package=(model_name+\"_Layer\"))\n",
    "class ConvQLayer(keras.layers.Layer):\n",
    "    \n",
    "\n",
    "    #replace the contents of qnode with experiment circuit\n",
    "    @qml.qnode(dev, interface='tf')\n",
    "    def q_node(inputs):\n",
    "        inputs *= np.pi\n",
    "        # Encoding of 4 classical input values\n",
    "        #Further testing of the AngleEmbedding function is needed\n",
    "        qml.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "        # Filter from arxiv.org/abs/2308.14930\n",
    "\n",
    "        qml.CNOT(wires=[1, 2])\n",
    "        qml.CNOT(wires=[0, 3])\n",
    "\n",
    "        # Measurement producing 4 classical output values\n",
    "        return [qml.expval(qml.PauliZ(j)) for j in range(n_qubits)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        #14x14 flattened 2x2 squares\n",
    "        get_subsections_14x14 = lambda im : tf.reshape(tf.unstack(tf.reshape(im,[14,2,14,2]), axis = 2),[14,14,4])\n",
    "        '''\n",
    "        example of sequential flat 28x28 indexes after above shuffle\n",
    "        [\n",
    "            [\n",
    "                [  0,   1,  28,  29],\n",
    "                [ 56,  57,  84,  85],\n",
    "                ...\n",
    "                [672, 673, 700, 701],\n",
    "                [728, 729, 756, 757]\n",
    "            ],\n",
    "            [\n",
    "                [  2,   3,  30,  31],\n",
    "                ...\n",
    "                [730, 731, 758, 759]\n",
    "            ],\n",
    "            ...\n",
    "            [\n",
    "                [ 26,  27,  54,  55],\n",
    "                ...\n",
    "                [754, 755, 782, 783]\n",
    "            ]\n",
    "        '''\n",
    "\n",
    "        #unpack 14x14 row by row\n",
    "        list_squares_2x2 = lambda image_subsections: tf.reshape(tf.unstack(image_subsections, axis = 1), [196,4])\n",
    "\n",
    "\n",
    "        #send 4 values to quantum function\n",
    "        process_square_2x2 = lambda square_2x2 : self.q_node(square_2x2)\n",
    "\n",
    "        #send all squares to the quantum function wrapper\n",
    "        process_subsections = lambda squares: tf.vectorized_map(process_square_2x2,squares)\n",
    "\n",
    "        #recompile the larger square\n",
    "        separate_channels = lambda channel_stack: tf.reshape(channel_stack, [14,14,4])\n",
    "        #each smaller square (channel) can be extracted as [:, :, channel]\n",
    "        \n",
    "        #apply function across batch\n",
    "        preprocessing = lambda input: tf.vectorized_map(\n",
    "            lambda image:(separate_channels(tf.transpose(process_subsections(list_squares_2x2(get_subsections_14x14(image)))))),\n",
    "            input\n",
    "        )\n",
    "\n",
    "        return preprocessing(inputs)\n",
    "\n",
    "qlayer = ConvQLayer()\n",
    "\n",
    "qlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrap preprocessing in model\n",
    "\n",
    "@keras.utils.register_keras_serializable(package=(model_name+\"_Pre_Model\"))\n",
    "def Pre_Model():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        qlayer\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "pre_model = Pre_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716157036.387770    1585 service.cc:145] XLA service 0x766f58013f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1716157036.387839    1585 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 2080, Compute Capability 7.5\n",
      "2024-05-19 22:17:16.434422: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-19 22:17:16.507064: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 16/938\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 11ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1716157037.513635    1585 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 1: saving model to Q_Model/epoch_checkpoints/cp-0001.weights.h5\n",
      "938/938 - 4s - 4ms/step - accuracy: 0.8615 - loss: 0.4791 - val_accuracy: 0.9277 - val_loss: 0.2615\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: saving model to Q_Model/epoch_checkpoints/cp-0002.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9271 - loss: 0.2515 - val_accuracy: 0.9385 - val_loss: 0.2166\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: saving model to Q_Model/epoch_checkpoints/cp-0003.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9369 - loss: 0.2156 - val_accuracy: 0.9425 - val_loss: 0.1979\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: saving model to Q_Model/epoch_checkpoints/cp-0004.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9424 - loss: 0.1968 - val_accuracy: 0.9445 - val_loss: 0.1878\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: saving model to Q_Model/epoch_checkpoints/cp-0005.weights.h5\n",
      "938/938 - 2s - 3ms/step - accuracy: 0.9456 - loss: 0.1848 - val_accuracy: 0.9464 - val_loss: 0.1818\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: saving model to Q_Model/epoch_checkpoints/cp-0006.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9479 - loss: 0.1764 - val_accuracy: 0.9473 - val_loss: 0.1778\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: saving model to Q_Model/epoch_checkpoints/cp-0007.weights.h5\n",
      "938/938 - 2s - 3ms/step - accuracy: 0.9499 - loss: 0.1700 - val_accuracy: 0.9479 - val_loss: 0.1750\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: saving model to Q_Model/epoch_checkpoints/cp-0008.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9510 - loss: 0.1650 - val_accuracy: 0.9481 - val_loss: 0.1731\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: saving model to Q_Model/epoch_checkpoints/cp-0009.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9521 - loss: 0.1609 - val_accuracy: 0.9489 - val_loss: 0.1717\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: saving model to Q_Model/epoch_checkpoints/cp-0010.weights.h5\n",
      "938/938 - 2s - 3ms/step - accuracy: 0.9532 - loss: 0.1576 - val_accuracy: 0.9493 - val_loss: 0.1707\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: saving model to Q_Model/epoch_checkpoints/cp-0011.weights.h5\n",
      "938/938 - 2s - 3ms/step - accuracy: 0.9540 - loss: 0.1547 - val_accuracy: 0.9491 - val_loss: 0.1700\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: saving model to Q_Model/epoch_checkpoints/cp-0012.weights.h5\n",
      "938/938 - 2s - 3ms/step - accuracy: 0.9546 - loss: 0.1522 - val_accuracy: 0.9498 - val_loss: 0.1695\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: saving model to Q_Model/epoch_checkpoints/cp-0013.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9552 - loss: 0.1500 - val_accuracy: 0.9496 - val_loss: 0.1692\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: saving model to Q_Model/epoch_checkpoints/cp-0014.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9556 - loss: 0.1481 - val_accuracy: 0.9495 - val_loss: 0.1691\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: saving model to Q_Model/epoch_checkpoints/cp-0015.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9563 - loss: 0.1464 - val_accuracy: 0.9497 - val_loss: 0.1690\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: saving model to Q_Model/epoch_checkpoints/cp-0016.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9567 - loss: 0.1449 - val_accuracy: 0.9497 - val_loss: 0.1691\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: saving model to Q_Model/epoch_checkpoints/cp-0017.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9572 - loss: 0.1435 - val_accuracy: 0.9499 - val_loss: 0.1692\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: saving model to Q_Model/epoch_checkpoints/cp-0018.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9575 - loss: 0.1422 - val_accuracy: 0.9497 - val_loss: 0.1694\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: saving model to Q_Model/epoch_checkpoints/cp-0019.weights.h5\n",
      "938/938 - 2s - 3ms/step - accuracy: 0.9576 - loss: 0.1411 - val_accuracy: 0.9498 - val_loss: 0.1697\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: saving model to Q_Model/epoch_checkpoints/cp-0020.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9578 - loss: 0.1400 - val_accuracy: 0.9499 - val_loss: 0.1699\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: saving model to Q_Model/epoch_checkpoints/cp-0021.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9580 - loss: 0.1390 - val_accuracy: 0.9503 - val_loss: 0.1702\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: saving model to Q_Model/epoch_checkpoints/cp-0022.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9583 - loss: 0.1381 - val_accuracy: 0.9503 - val_loss: 0.1706\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: saving model to Q_Model/epoch_checkpoints/cp-0023.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9586 - loss: 0.1372 - val_accuracy: 0.9506 - val_loss: 0.1709\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: saving model to Q_Model/epoch_checkpoints/cp-0024.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9590 - loss: 0.1364 - val_accuracy: 0.9506 - val_loss: 0.1713\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: saving model to Q_Model/epoch_checkpoints/cp-0025.weights.h5\n",
      "938/938 - 2s - 3ms/step - accuracy: 0.9592 - loss: 0.1357 - val_accuracy: 0.9506 - val_loss: 0.1716\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: saving model to Q_Model/epoch_checkpoints/cp-0026.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9595 - loss: 0.1350 - val_accuracy: 0.9504 - val_loss: 0.1720\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: saving model to Q_Model/epoch_checkpoints/cp-0027.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9596 - loss: 0.1343 - val_accuracy: 0.9501 - val_loss: 0.1724\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: saving model to Q_Model/epoch_checkpoints/cp-0028.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9598 - loss: 0.1337 - val_accuracy: 0.9500 - val_loss: 0.1727\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: saving model to Q_Model/epoch_checkpoints/cp-0029.weights.h5\n",
      "938/938 - 2s - 2ms/step - accuracy: 0.9598 - loss: 0.1331 - val_accuracy: 0.9503 - val_loss: 0.1731\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: saving model to Q_Model/epoch_checkpoints/cp-0030.weights.h5\n",
      "938/938 - 2s - 3ms/step - accuracy: 0.9600 - loss: 0.1325 - val_accuracy: 0.9501 - val_loss: 0.1735\n"
     ]
    }
   ],
   "source": [
    "#core model\n",
    "\n",
    "@keras.utils.register_keras_serializable(package=(model_name+\"_Core_Model\"))\n",
    "def Q_Model():\n",
    "    \"\"\"Initializes and returns a custom Keras model\n",
    "    which is ready to be trained.\"\"\"\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "q_model = Q_Model()\n",
    "\n",
    "#preprocessing\n",
    "pre_train_images = pre_model.predict(train_images,batch_size=n_batches)\n",
    "pre_test_images = pre_model.predict(test_images,batch_size=n_batches)\n",
    "\n",
    "#training\n",
    "q_history = q_model.fit(\n",
    "    pre_train_images,\n",
    "    train_labels,\n",
    "    validation_data=(pre_test_images, test_labels),\n",
    "    batch_size = n_batches,\n",
    "    epochs=n_epochs,\n",
    "    verbose=2, callbacks=[tensorboard_callback, cp_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#example of quantum preprocessed(filtered) chanels\n",
    "image_titles = ['Five-Ch1', 'Five-Ch2', 'Five-Ch3','Five-Ch4']\n",
    "f, ax = plt.subplots(nrows=1, ncols=4, figsize=(12, 4))\n",
    "for i, title in enumerate(image_titles):\n",
    "    ax[i].set_title(title, fontsize=16)\n",
    "    ax[i].imshow(pre_train_images[0][:,:,i])\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 4 0 9 1 1 2 4 3 2 7 3 8 6 9 0 5 6\n",
      " 0 7 6 1 8 7 9 3 9 8 5 5 3 3 0 7 4 9 8 0 9 4 1 9 4 6 0]\n",
      "[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 4 0 9 1 1 2 4 3 2 7 3 8 6 9 0 5 6\n",
      " 0 7 6 1 8 7 9 3 9 8 5 9 3 3 0 7 4 9 8 0 9 4 1 4 4 6 0]\n",
      "[Errno 17] File exists: 'Q_Model'\n",
      "INFO:tensorflow:Assets written to: Q_Model/Q_Model.keras/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Q_Model/Q_Model.keras/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'Q_Model/Q_Model.keras'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor_5')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  130224755052176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130224755054480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#compare predictions with labels\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "full_model = keras.models.Sequential([pre_model,q_model], name=model_name)\n",
    "\n",
    "pred = full_model.predict(train_images[:n_batches], batch_size=n_batches)\n",
    "\n",
    "print(np.argmax(pred, axis=1))\n",
    "\n",
    "print(train_labels[:n_batches])\n",
    "\n",
    "\n",
    "\n",
    "model_path = model_name + \"/\" + model_name + \".keras\"\n",
    "model_dir = os.path.dirname(model_path)\n",
    "\n",
    "try:  \n",
    "    os.mkdir(model_dir)  \n",
    "except OSError as error:  \n",
    "    print(error)   \n",
    "\n",
    "full_model.export(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 4 0 9 1 1 2 4 3 2 7 3 8 6 9 0 5 6\n",
      " 0 7 6 1 8 7 9 3 9 8 5 5 3 3 0 7 4 9 8 0 9 4 1 9 4 6 0]\n"
     ]
    }
   ],
   "source": [
    "restored_as_layer = keras.layers.TFSMLayer(model_path, call_endpoint=\"serve\")\n",
    "\n",
    "restored_model = keras.models.Sequential([restored_as_layer], name=(\"Restored_\"+model_name))\n",
    "\n",
    "#view predictions of re-loaded model\n",
    "\n",
    "print(np.argmax(restored_model.predict(train_images[:n_batches],batch_size=n_batches), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Restored_Q_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Restored_Q_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ tfsm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TFSMLayer</span>)          │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ tfsm_layer (\u001b[38;5;33mTFSMLayer\u001b[0m)          │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m10\u001b[0m)               │         \u001b[38;5;34m7,850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,850</span> (30.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,850\u001b[0m (30.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,850</span> (30.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,850\u001b[0m (30.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "restored_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML-QPF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
